---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka-python-inference-app-fastembed-gpu
  namespace: ${K8S_NAMESPACE}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-python-inference-app-fastembed-gpu
  template:
    metadata:
      labels:
        app: kafka-python-inference-app-fastembed-gpu
    spec:
      containers:
        - name: kafka-python-inference-app-fastembed-gpu
          image: ${DOCKER_IMAGE_URL}kafka-python-inference-app:latest-fastembed-gpu
          imagePullPolicy: Always
          env:
            - name: BOOTSTRAP_SERVER
              value: ${BOOTSTRAP_SERVER}
            - name: SCHEMA_REGISTRY
              value: ${SCHEMA_REGISTRY}
            - name: INPUT_TOPIC
              value: "inference-test-paper"
            - name: OUTPUT_TOPIC
              value: "inference-test-embedded-paper"
            - name: EXECUTION_PROVIDERS
              value: "CUDAExecutionProvider,CPUExecutionProvider"
            - name: EMBEDDING_MODEL
              value: "BAAI/bge-large-en-v1.5"
            - name: CHUNK_SIZE
              value: "512"
            - name: CHUNK_OVERLAP
              value: "32"
            - name: BATCH_SIZE
              value: "32"
            - name: LOG_LEVEL
              value: "INFO"
          resources:
            requests:
              cpu: "1"
              memory: "2Gi"
              nvidia.com/gpu: "1"
            limits:
              cpu: "3"
              memory: "8Gi"
              nvidia.com/gpu: "1"
      nodeSelector:
        cloud.google.com/gke-accelerator: nvidia-l4
